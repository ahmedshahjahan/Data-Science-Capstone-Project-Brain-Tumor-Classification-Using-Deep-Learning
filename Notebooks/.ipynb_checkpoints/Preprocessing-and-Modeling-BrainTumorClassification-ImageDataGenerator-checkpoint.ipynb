{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Modeling (Brain Tumor Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, sys, os.path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.layers import Dense, Flatten, Conv2D, BatchNormalization, MaxPool2D, MaxPooling2D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating the training, test, and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4923 images belonging to 4 classes.\n",
      "Found 1050 images belonging to 4 classes.\n",
      "Found 1050 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating Dataset using ImageDataGenerator\n",
    "# Using Resized and Cleaned Dataset\n",
    "# From \"../BrainTumorDataClean/\" directory\n",
    "\n",
    "#train = ImageDataGenerator(rescale=1/255)\n",
    "#test = ImageDataGenerator(rescale=1/255)\n",
    "#validation = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train = ImageDataGenerator(rescale=1/255, horizontal_flip=True)\n",
    "\n",
    "test = ImageDataGenerator(rescale=1/255, horizontal_flip=True) \n",
    "\n",
    "validation = ImageDataGenerator(rescale=1/255, horizontal_flip=True)\n",
    "\n",
    "\n",
    "train_dataset = train.flow_from_directory(\"../BrainTumorDataClean/Training\",\n",
    "                                          target_size=(128,128),\n",
    "                                          batch_size = 32,\n",
    "                                          class_mode = 'categorical', color_mode=\"grayscale\")\n",
    "\n",
    "validation_dataset = validation.flow_from_directory(\"../BrainTumorDataClean/Validation\",\n",
    "                                          target_size=(128,128),\n",
    "                                          batch_size =32,\n",
    "                                          class_mode = 'categorical', color_mode=\"grayscale\")\n",
    "\n",
    "test_dataset = test.flow_from_directory(\"../BrainTumorDataClean/Testing\",\n",
    "                                          target_size=(128,128),\n",
    "                                          batch_size =32,\n",
    "                                          class_mode = 'categorical', color_mode=\"grayscale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Glioma': 0, 'Meningioma': 1, 'NoTumor': 2, 'Pituitary': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Glioma': 0, 'Meningioma': 1, 'NoTumor': 2, 'Pituitary': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Glioma': 0, 'Meningioma': 1, 'NoTumor': 2, 'Pituitary': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 2, 0, 1, 2, 1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label =next(test_dataset)\n",
    "np.argmax(label[:10], axis=-1)\n",
    "#for i, j in zip(img, label):\n",
    "#     print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Implementing a Simple Neural Network using  dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the Simple model using Dense Layer [optimizer = 'adam']\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Flatten(input_shape=(128,128,1)))\n",
    "\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "\n",
    "model1.add(Dense(256, activation='relu'))\n",
    "\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "\n",
    "model1.add(Dense(16, activation='relu'))\n",
    "\n",
    "model1.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with learning_rate=0.001\n",
    "early_stopping_monitor1 = EarlyStopping(patience=4)\n",
    "model1.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history1 = model1.fit(train_dataset, epochs = 20, batch_size=32, \\\n",
    "                      validation_data = validation_dataset, callbacks=[early_stopping_monitor1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(predictions1, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = test_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (Implementing a Simple Convolutional Neural Network Model Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(128,128,1)))\n",
    "model2.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model2.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model2.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor2 = EarlyStopping(patience=4)\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(train_dataset, steps_per_epoch=4923//32, validation_data=validation_dataset,\\\n",
    "           validation_steps=len(validation_dataset)//32, epochs=20, callbacks = [early_stopping_monitor2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 =model2.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Historical LeNet-5 Architecture\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(20, (5,5), activation='relu', padding = 'same', strides=1, input_shape = (128,128,1)))\n",
    "model3.add(MaxPool2D((2,2), strides=2))\n",
    "\n",
    "model3.add(Conv2D(50, (5,5), activation='relu', strides=1, padding = 'same'))\n",
    "model3.add(MaxPool2D((2,2), strides=2))\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "\n",
    "model3.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "early_stopping_monitor = EarlyStopping(patience=4)\n",
    "model3.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model3.fit(train_dataset, epochs = 20, batch_size=10, \\\n",
    "                      validation_data = validation_dataset, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model3.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 96.95%\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model4.add(Conv2D(32,(3,3), input_shape=(128,128,1), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional layer 2\n",
    "model4.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "# Neural network\n",
    "\n",
    "model4.add(Dense(units= 252, activation='relu'))\n",
    "model4.add(Dropout(0.2))\n",
    "\n",
    "model4.add(Dense(units=252, activation='relu'))\n",
    "model4.add(Dropout(0.2))\n",
    "\n",
    "model4.add(Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.0001, clipvalue=0.5)\n",
    "model4.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "                   metrics= ['categorical_accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model6.fit(train_dataset, steps_per_epoch=4923//32, epochs=20, validation_data=validation_dataset, validation_steps= 1050//32,\n",
    "                     callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = model4.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5 Implementing complex CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying Model Architecture (Convolutional Neural Network)\n",
    "model5 = Sequential()\n",
    "\n",
    "# Convolutional layer, Batch Normalization layer, and maxpool layer 1\n",
    "model5.add(Conv2D(32,(3,3),activation='relu',input_shape=(128,128,1)))\n",
    "# model5.add(BatchNormalization())\n",
    "model5.add(MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer,  Batch Normalization layer, and maxpool layer 2\n",
    "model5.add(Conv2D(64,(3,3),activation='relu'))\n",
    "# model5.add(BatchNormalization())\n",
    "model5.add(MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer,  Batch Normalization layer, and maxpool layer 3\n",
    "model5.add(Conv2D(128,(3,3),activation='relu'))\n",
    "# model5.add(BatchNormalization())\n",
    "model5.add(MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer,  Batch Normalization layer, and maxpool layer 4\n",
    "model5.add(Conv2D(256,(3,3),activation='relu'))\n",
    "# model5.add(BatchNormalization())\n",
    "model5.add(MaxPool2D(2,2))\n",
    "\n",
    "# This layer flattens the resulting image array to 1D array\n",
    "model5.add(Flatten())\n",
    "\n",
    "# Hidden layer with 512 neurons and Rectified Linear Unit activation function \n",
    "model5.add(Dense(512,activation='relu'))\n",
    "# model5.add(Dropout(0.25))\n",
    "\n",
    "# Output layer with single neuron which gives 0 for Cat or 1 for Dog \n",
    "#Here we use sigmoid activation function which makes our model output to lie between 0 and 1\n",
    "model5.add(Dense(4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=4)\n",
    "model5.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps_per_epoch = train_imagesize/batch_size\n",
    "\n",
    "model5.fit(train_dataset, epochs = 20, batch_size=32, validation_data = validation_dataset, \\\n",
    "           callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5 = model5.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(predictions5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model6.add(Conv2D(64,(7,7), input_shape=(128,128, 1), padding='same', activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Convolutional layer 2\n",
    "model6.add(Conv2D(128,(7,7), padding='same', activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional layer 3\n",
    "model6.add(Conv2D(256,(7,7), padding='same', activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional layer 4\n",
    "model6.add(Conv2D(512,(7,7), padding='same', activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model6.add(Flatten())\n",
    "\n",
    "# Full connect layers\n",
    "\n",
    "model6.add(Dense(units= 512, activation='relu'))\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "model6.add(Dense(units=512, activation='relu'))\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "model6.add(Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.0001, clipvalue=0.5)\n",
    "model6.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "                   metrics= ['categorical_accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history6 = model6.fit(train_dataset, steps_per_epoch=4923//32, epochs=20, validation_data=validation_dataset, validation_steps= 1050//32,\n",
    "                     callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5 =model5.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Models Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset, validation_dataset, test_dataset\n",
    "model1, history1, predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "y_test_new = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(predictions2, axis=-1)\n",
    "y_true = test_dataset.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred.shape\n",
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true.shape\n",
    "y_true[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(14,7))\n",
    "sns.heatmap(confusion_matrix(y_true,Y_pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True,\n",
    "           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\n",
    "fig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n",
    "             fontname='monospace',color=colors_dark[1],y=0.92,x=0.28,alpha=0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "# type(history1)\n",
    "# history1.history.items()\n",
    "plt.plot(history1.history['loss'], 'b', label='training loss')\n",
    "plt.plot(history1.history['val_loss'], 'r', label='validation loss')\n",
    "plt.xlabel('Epochs', )\n",
    "plt.ylabel('Validation score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_dataset.classes, y_pred=np.argmax(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['Glioma', 'Meningioma', 'NoTumor', 'Pituitary']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/2607\n",
    "test_generator = ImageDataGenerator()\n",
    "test_data_generator = test_generator.flow_from_directory(\n",
    "    test_data_path, # Put your path here\n",
    "     target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    shuffle=False)\n",
    "test_steps_per_epoch = numpy.math.ceil(test_data_generator.samples / test_data_generator.batch_size)\n",
    "\n",
    "predictions = model.predict_generator(test_data_generator, steps=test_steps_per_epoch)\n",
    "# Get most likely class\n",
    "predicted_classes = numpy.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = test_data_generator.classes\n",
    "class_labels = list(test_data_generator.class_indices.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = ['Glioma','Meningioma','NoTumor','Pituitary']\n",
    "def get_all_metrics(y_true, y_pred, model_num, history):\n",
    "    \"\"\"\n",
    "    Confusion matrix and model performance metric visualization\n",
    "    \"\"\"\n",
    "    y_true = np.argmax(y_true, axis=-1)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    print(f\"\\nMax. training accuracy for {model_num}: {max(history1.history['accuracy'])*100:.2f}%\")\n",
    "    print(f\"Max. validation accuracy for {model_num}: {max(history1.history['val_accuracy'])*100:.2f}%\")\n",
    "    accuracy = np.sum(y_true == y_pred)/len(y_true)\n",
    "    print(f\"Test accuracy for {model_num}: {(accuracy*100):.2f}%\")\n",
    "    \n",
    "    print(f\"\\nClassification Report for {model_num}:\\n\\n\", classification_report(y_true, y_pred))\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(16,8))\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True,\n",
    "           cmap=\"Greens\",alpha=0.7,linewidths=2,linecolor='gray',fmt='d')\n",
    "    fig.text(s=f'      Confusion Matrix for {model_num}',size=18,fontweight='bold',\n",
    "            fontname='monospace',color='g',y=0.92,x=0.28,alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_metrics(y_test, y_pred1, 'model_1', history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "from keras.models import model_from_json\n",
    "model_json = classifier.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize weights to HDF5\n",
    "classifier.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for keras nodel\n",
    "train_img = []\n",
    "train_labels = []\n",
    "\n",
    "test_img = []\n",
    "test_labels = []\n",
    "\n",
    "path_train = ('/kaggle/input/brain-tumor-classification-mri/Training/')\n",
    "path_test = ('/kaggle/input/brain-tumor-classification-mri/Testing/')\n",
    "img_size= 300\n",
    "\n",
    "for i in os.listdir(path_train):\n",
    "    for j in os.listdir(path_train+i):\n",
    "        train_img.append (cv2.resize(cv2.imread(path_train+i+'/'+j), (img_size,img_size))) \n",
    "        train_labels.append(i)\n",
    "        \n",
    "for i in os.listdir(path_test):\n",
    "    for j in os.listdir(path_test+i):\n",
    "        test_img.append (cv2.resize(cv2.imread(path_test+i+'/'+j), (img_size,img_size))) \n",
    "        test_labels.append(i)\n",
    "        \n",
    "train_img = (np.array(train_img))\n",
    "test_img = (np.array(test_img))\n",
    "train_labels_encoded = [0 if category == 'no_tumor' else(1 if category == 'glioma_tumor' else(2 if category=='meningioma_tumor' else 3)) for category in list(train_labels)]\n",
    "test_labels_encoded = [0 if category == 'no_tumor' else(1 if category == 'glioma_tumor' else(2 if category=='meningioma_tumor' else 3)) for category in list(test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "#steps_per_epoch = train_imagesize/batch_size\n",
    "result = 0\n",
    "model_evaluate = []\n",
    "start = time.time()\n",
    "for i in range(5):\n",
    "    model.fit(train_dataset,\n",
    "             epochs = 10, batch_size=10,\n",
    "             validation_data = 0.0  \n",
    "             )\n",
    "    result = model.evaluate(test_dataset)\n",
    "    model_evaluate.append(result[1])\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RMSprop(\n",
    "    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop', **kwargs\n",
    ")\n",
    "Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam', **kwargs\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 300\n",
    "effnet = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))\n",
    "model = effnet.output\n",
    "model = tf.keras.layers.GlobalAveragePooling2D()(model)\n",
    "model = tf.keras.layers.Dropout(rate=0.5)(model)\n",
    "model = tf.keras.layers.Dense(4,activation='softmax')(model)\n",
    "model = tf.keras.models.Model(inputs=effnet.input, outputs = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "\n",
    "model = ...\n",
    "\n",
    "visualkeras.layered_view(model).show() # display using your system viewer\n",
    "visualkeras.layered_view(model, to_file='output.png') # write to disk\n",
    "visualkeras.layered_view(model, to_file='output.png').show() # write and show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont\n",
    "\n",
    "font = ImageFont.truetype(\"arial.ttf\", 32)  # using comic sans is strictly prohibited!\n",
    "visualkeras.layered_view(model, legend=True, font=font)  # font is optional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/visualkeras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "'''\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "'''\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Individual predictions\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/OIP.jpg', target_size = (512, 512)) # Cargamos la imagen con un tama√±o igual a\n",
    "                                                                                           # los anteriores\n",
    "test_image = image.img_to_array(test_image) # Convertimos la imagen en un array\n",
    "test_image = np.expand_dims(test_image, axis = 0) # Modificamos las dimensions\n",
    "result = classifier.predict(test_image) # Prediccion\n",
    "print(training_dataset.class_indices)\n",
    "print(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "'''\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary 1: horizontalflip=True, (64,64)\n",
    "model 1=0.88 , model2=0.9419, model3=0.9695, model4=0.9771, model5=0.9629, model6=0.9695\n",
    "Summary 2: (64,64)\n",
    "model 1=0.8990 , model2=0.9333, model3=0.9600, model4=0.9610, model5=, model6=0.9533\n",
    "Summary 3: horizontalflip=True, (128,128)\n",
    "model 1=0.84 , model2=0.9162, model3=0.9790, model4=0.9657, model5=, model6=0.9552\n",
    "Summary 4: (128,128)\n",
    "model 1=0.92 , model2=0.9467, model3=0.9638, model4=0.8971, model5=, model6=0.9457"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
